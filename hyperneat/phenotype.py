import numpy as np
import torch
import torch.nn as nn
from .substrate import grid_positions, output_positions, pair_features

class Phenotype(nn.Module):
    """
    Phenotype network constructed from a CPPN genome and substrate configuration.

    The phenotype is a feedforward neural network with:
      - Input layer based on substrate input grid positions
      - Hidden layer based on substrate hidden grid positions
      - Output layer based on defined output positions
    Connections and weights are generated by querying the genome (CPPN).
    """

    def __init__(self, genome, substrate_cfg):
        super().__init__()
        self.genome = genome
        cfg = substrate_cfg

        # Define neuron positions for each layer
        self.in_pos = grid_positions(cfg['input_w'], cfg['input_h'], cfg.get('input_channels', 3))
        self.hid_pos = grid_positions(cfg['hidden_w'], cfg['hidden_h'], channels=1)
        self.out_pos = output_positions(cfg['output_dim'])
        self.weight_threshold = cfg.get('weight_threshold', 0.01)

        # Input → Hidden weights
        feats_ih = pair_features(self.in_pos, self.hid_pos)
        w_ih = genome.forward(feats_ih).reshape(len(self.in_pos), len(self.hid_pos))
        w_ih = np.where(np.abs(w_ih) >= self.weight_threshold, w_ih, 0.0)

        # Hidden → Output weights
        feats_ho = pair_features(self.hid_pos, self.out_pos)
        w_ho = genome.forward(feats_ho).reshape(len(self.hid_pos), len(self.out_pos))
        w_ho = np.where(np.abs(w_ho) >= self.weight_threshold, w_ho, 0.0)

        # Store layer sizes
        self.N_in = len(self.in_pos)
        self.N_hid = len(self.hid_pos)
        self.N_out = len(self.out_pos)

        # Build network layers (no bias, weights defined by CPPN)
        self.lin1 = nn.Linear(self.N_in, self.N_hid, bias=False)
        self.lin2 = nn.Linear(self.N_hid, self.N_out, bias=False)

        # Copy generated weights into torch layers
        with torch.no_grad():
            self.lin1.weight.copy_(torch.from_numpy(w_ih.T.astype(np.float32)))
            self.lin2.weight.copy_(torch.from_numpy(w_ho.T.astype(np.float32)))

        self.act = nn.Tanh()

    def forward(self, x):
        """
        Forward pass through phenotype network.

        Args:
            x (torch.Tensor): Input image batch of shape (B, C, H, W).

        Returns:
            torch.Tensor: Network output of shape (B, N_out).
        """
        B = x.shape[0]

        # Flatten input into (B, N_in) while preserving batch dimension
        xb = x.permute(0, 1, 2, 3).contiguous().view(B, -1)

        # Adjust dimensionality if mismatch due to channel/size differences
        if xb.shape[1] != self.N_in:
            if xb.shape[1] > self.N_in:
                xb = xb[:, :self.N_in]
            else:
                pad = torch.zeros((B, self.N_in - xb.shape[1]), dtype=xb.dtype, device=xb.device)
                xb = torch.cat([xb, pad], dim=1)

        # Feedforward pass
        h = self.act(self.lin1(xb))
        out = self.lin2(h)
        return out
